{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham Folder: 1259046, Spam Folder: 1315130, Total Files: 10347, Data colledcted in Files: 10347\n"
     ]
    }
   ],
   "source": [
    "#Read Mails\n",
    "mails_path = 'Mail_Test'\n",
    "\n",
    "spam = []\n",
    "ham = []\n",
    "lines = []\n",
    "file_data = []\n",
    "separator = '/'\n",
    "files = -1\n",
    "for (path,dirname,filename) in os.walk(mails_path):\n",
    "    for counter, i in enumerate(filename):\n",
    "        #print(i)\n",
    "        if i.endswith('.ham.txt'):\n",
    "            ham_path = path + separator + i\n",
    "            with open(ham_path) as f:\n",
    "                try:\n",
    "                    files += 1\n",
    "                    #print(counter)\n",
    "                    file_data.insert(counter, [[], 'ham'])\n",
    "                    #print(file_data)\n",
    "                    for i, line in enumerate(f):\n",
    "                        if i != 0: ## Ignore Subject Line\n",
    "                            words = line.split()\n",
    "                            file_data[counter][0] += words\n",
    "                            #lines.append(words)\n",
    "                            ham += words         \n",
    "                    #print(file_data)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "        elif i.endswith('.spam.txt'):\n",
    "            spam_path = path + separator + i\n",
    "            with open(spam_path) as f:\n",
    "                try:\n",
    "                    files += 1\n",
    "                    file_data.insert(counter, [[], 'spam'])\n",
    "                    for i, line in enumerate(f):\n",
    "                        if i != 0: ## Ignore Subject Line\n",
    "                            words = line.split()\n",
    "                            file_data[counter][0] += words\n",
    "                            #lines.append(words)\n",
    "                            spam += words\n",
    "                except:\n",
    "                    pass\n",
    "#print(file_data[0], file_data[2])                    \n",
    "print('Ham Folder: {}, Spam Folder: {}, Total Files: {}, Data colledcted in Files: {}'.format(len(ham), len(spam), files+1, len(file_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('-', 107017), ('.', 51914), ('/', 51498), (',', 44336), ('the', 31720), (':', 27089), ('to', 25919), ('ect', 21689), ('@', 17978), ('and', 15184)]\n",
      "[('.', 73520), (',', 42568), ('_', 35999), ('-', 32445), ('the', 30881), ('to', 26233), ('and', 21833), ('of', 19394), (':', 16670), ('a', 16121)]\n"
     ]
    }
   ],
   "source": [
    "# Count the occurences of letters\n",
    "all_count_ham = Counter(ham)\n",
    "print (all_count_ham.most_common(10))\n",
    "\n",
    "all_count_spam = Counter(spam)\n",
    "print (all_count_spam.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 4, 5], [3, 5, 6]]\n",
      "Ham Folder: 500910, Spam Folder: 587250, Total Files: 10346\n"
     ]
    }
   ],
   "source": [
    "# Remove Junk characters and common characters\n",
    "eng_stopwords = stopwords.words('english')\n",
    "punctuation_marks = list(punctuation)\n",
    "\n",
    "ham_stopwordless = list(filter(lambda x: x not in eng_stopwords and x not in punctuation_marks and x.isalpha() == True and len(x) != 1, ham))\n",
    "spam_stopwordless = list(filter(lambda x: x not in eng_stopwords and x not in punctuation_marks and x.isalpha() == True and len(x) != 1, spam))\n",
    "\n",
    "a = [[1,2,4,5], [3,5,6]]\n",
    "print(list(filter(lambda x: x not in [1,6] , a)))\n",
    "print('Ham Folder: {}, Spam Folder: {}, Total Files: {}'.format(len(ham_stopwordless), len(spam_stopwordless), files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Ham Most Common Words----------\n",
      " \n",
      "[('ect', 21689), ('enron', 11106), ('hou', 10252), ('please', 4032), ('subject', 3665), ('cc', 3238), ('pm', 3201), ('com', 3125), ('gas', 3009), ('deal', 2894), ('corp', 2549), ('thanks', 2480), ('know', 2149), ('meter', 2132), ('need', 1930), ('daren', 1906), ('forwarded', 1828), ('hpl', 1806), ('would', 1780), ('let', 1602), ('new', 1539), ('attached', 1444), ('may', 1380), ('mmbtu', 1350), ('see', 1310), ('time', 1282), ('get', 1251), ('houston', 1209), ('day', 1205), ('questions', 1177), ('ena', 1169), ('business', 1168), ('sally', 1162), ('farmer', 1161), ('information', 1155), ('also', 1095), ('risk', 1068), ('lon', 1032), ('management', 1020), ('xls', 1015), ('us', 1013), ('robert', 1002), ('price', 998), ('call', 992), ('th', 971), ('one', 970), ('following', 962), ('contract', 933), ('bob', 927), ('sent', 924), ('month', 920), ('volume', 913), ('texas', 896), ('energy', 895), ('message', 887), ('meeting', 870), ('mail', 856), ('sitara', 850), ('deals', 849), ('trading', 840), ('global', 837), ('like', 831), ('mary', 796), ('volumes', 780), ('process', 772), ('group', 770), ('week', 759), ('pec', 752), ('report', 745), ('office', 710), ('team', 707), ('change', 705), ('file', 697), ('work', 688), ('system', 682), ('back', 678), ('list', 661), ('mike', 659), ('help', 657), ('flow', 656), ('review', 651), ('next', 650), ('daily', 647), ('well', 644), ('production', 641), ('david', 636), ('operations', 632), ('john', 624), ('date', 623), ('na', 622), ('original', 620), ('gary', 614), ('think', 613), ('want', 612), ('make', 611), ('company', 609), ('could', 607), ('development', 604), ('mark', 602), ('per', 600)]\n",
      " \n",
      "---------Spam Most Common Words----------\n",
      "               \n",
      "[('com', 3827), ('email', 2834), ('http', 2698), ('company', 2617), ('information', 2310), ('business', 2284), ('please', 2165), ('one', 2014), ('free', 1922), ('www', 1916), ('us', 1898), ('click', 1793), ('mail', 1784), ('money', 1756), ('get', 1728), ('time', 1689), ('new', 1672), ('may', 1631), ('make', 1404), ('de', 1388), ('adobe', 1388), ('message', 1376), ('website', 1322), ('account', 1233), ('statements', 1216), ('like', 1205), ('report', 1180), ('within', 1177), ('price', 1162), ('online', 1160), ('address', 1156), ('best', 1128), ('order', 1111), ('list', 1103), ('people', 1098), ('want', 1092), ('site', 1069), ('software', 1063), ('send', 1039), ('net', 1015), ('receive', 1010), ('use', 971), ('need', 970), ('offer', 965), ('see', 938), ('would', 936), ('investment', 914), ('name', 912), ('save', 886), ('internet', 883), ('also', 876), ('life', 874), ('future', 868), ('million', 835), ('way', 813), ('security', 809), ('inc', 808), ('many', 803), ('could', 802), ('know', 798), ('market', 792), ('home', 792), ('web', 786), ('today', 769), ('looking', 766), ('made', 764), ('day', 761), ('contact', 758), ('go', 758), ('products', 755), ('marketing', 752), ('companies', 744), ('stock', 740), ('take', 730), ('work', 726), ('number', 710), ('news', 707), ('forward', 706), ('received', 702), ('first', 691), ('program', 689), ('service', 676), ('per', 665), ('subject', 665), ('help', 663), ('prices', 658), ('good', 656), ('search', 650), ('securities', 649), ('world', 648), ('without', 645), ('days', 633), ('product', 628), ('professional', 627), ('services', 623), ('available', 621), ('even', 618), ('system', 617), ('high', 608), ('right', 606)]\n",
      "---------Most Common Words----------\n",
      " \n",
      "[('ect', 21695), ('enron', 11106), ('hou', 10293), ('com', 6952), ('please', 6197), ('subject', 4330), ('gas', 3527), ('information', 3465), ('business', 3452), ('email', 3339), ('pm', 3300), ('cc', 3260), ('company', 3226), ('new', 3211), ('deal', 3083), ('http', 3012), ('may', 3011), ('one', 2984), ('get', 2979), ('time', 2971), ('know', 2947), ('us', 2911), ('need', 2900), ('thanks', 2746), ('would', 2716), ('corp', 2710), ('mail', 2640), ('message', 2263), ('see', 2248), ('free', 2211), ('price', 2160), ('meter', 2134), ('www', 2129), ('click', 2084), ('like', 2036), ('make', 2015), ('also', 1971), ('day', 1966), ('let', 1931), ('report', 1925), ('daren', 1907), ('money', 1849), ('forwarded', 1839), ('hpl', 1806), ('list', 1764), ('want', 1704), ('net', 1568), ('order', 1552), ('people', 1543), ('attached', 1515), ('within', 1502), ('sent', 1499), ('call', 1468), ('website', 1440), ('address', 1433), ('following', 1429), ('de', 1428), ('use', 1426), ('account', 1424), ('risk', 1418), ('work', 1414), ('could', 1409), ('best', 1401), ('send', 1400), ('management', 1400), ('adobe', 1390), ('th', 1378), ('name', 1361), ('month', 1357), ('questions', 1356), ('mmbtu', 1350), ('contact', 1348), ('today', 1334), ('help', 1320), ('system', 1299), ('online', 1299), ('forward', 1289), ('number', 1278), ('per', 1265), ('receive', 1259), ('statements', 1255), ('next', 1243), ('first', 1230), ('houston', 1229), ('go', 1223), ('site', 1213), ('take', 1211), ('week', 1183), ('farmer', 1172), ('energy', 1170), ('sally', 1169), ('ena', 1169), ('days', 1167), ('made', 1161), ('group', 1142), ('market', 1134), ('received', 1128), ('offer', 1127), ('software', 1126), ('well', 1124)]\n"
     ]
    }
   ],
   "source": [
    "# Count the occurences of letters\n",
    "print ('---------Ham Most Common Words----------')\n",
    "print (' ')\n",
    "all_count_ham = Counter(ham_stopwordless)\n",
    "print (all_count_ham.most_common(100))\n",
    "print (' ')\n",
    "print ('---------Spam Most Common Words----------')\n",
    "print ('               ')\n",
    "all_count_spam = Counter(spam_stopwordless)\n",
    "print (all_count_spam.most_common(100))\n",
    "\n",
    "#join lists\n",
    "list_combined = ham_stopwordless + spam_stopwordless\n",
    "print ('---------Most Common Words----------')\n",
    "print (' ')\n",
    "\n",
    "all_count = Counter(list_combined)\n",
    "features = all_count.most_common(3000)\n",
    "\n",
    "print(all_count.most_common(100))\n",
    "#print(features[12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10347, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Build Feature Matrix\n",
    "X_features = np.zeros((files+1, 3000))\n",
    "Y_features = []\n",
    "print (X_features.shape)\n",
    "\n",
    "for counter, i in enumerate(file_data):\n",
    "    #print(counter, i)\n",
    "    for counter_f, j in enumerate(features):\n",
    "        X_features[counter][counter_f] = i[0].count(j[0])\n",
    "    Y_features.insert(counter, i[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7760, 3000) (2587, 3000) 7760 2587\n"
     ]
    }
   ],
   "source": [
    "# Training and Test Data Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_features, Y_features, test_size = 0.25, random_state = 42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, len(Y_train), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97100889060688056"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rand_forest = RandomForestClassifier(random_state = 42, min_samples_leaf = 1, min_samples_split = 5) #Obtained from GridSearchCV\n",
    "clf_rand_forest.fit(X_train, Y_train)\n",
    "clf_rand_forest.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9311944337069965"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM Classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_svc = SVC(random_state = 1)\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "clf_svc.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90916119056822575"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf_nb = GaussianNB()\n",
    "clf_nb.fit(X_train, Y_train)\n",
    "clf_nb.score(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
